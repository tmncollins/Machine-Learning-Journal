{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":12189904,"sourceType":"datasetVersion","datasetId":7678100},{"sourceId":13148027,"sourceType":"datasetVersion","datasetId":8263542}],"dockerImageVersionId":31089,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/tmncollins/ml-journal-7-boiling-point-predictor?scriptVersionId=281498707\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"code","source":"try:\n    import rdkit\nexcept:\n    !pip install --quiet /kaggle/input/rdkit-2025-3-3-cp311/rdkit-2025.3.3-cp311-cp311-manylinux_2_28_x86_64.whl\n\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nimport rdkit\n\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\nimport sys\nfrom tqdm.notebook import tqdm\n\nimport networkx as nx\nfrom rdkit.Chem import AllChem\nfrom rdkit.Chem import Descriptors\nfrom rdkit.Chem import rdmolops\nfrom rdkit import Chem\nimport pickle\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:31:36.454393Z","iopub.execute_input":"2025-11-24T18:31:36.455252Z","iopub.status.idle":"2025-11-24T18:31:36.461836Z","shell.execute_reply.started":"2025-11-24T18:31:36.455222Z","shell.execute_reply":"2025-11-24T18:31:36.461251Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IMPORT DATASET - FINGERPRINT\ndf_train = (pd.read_csv('/kaggle/input/smiles-boiling-point/solvents_bp.csv')).dropna()\n\ndf_save = df_train.dropna().drop_duplicates(subset=['smiles'])\ndf_save.to_csv('bp_data.csv')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:31:36.463141Z","iopub.execute_input":"2025-11-24T18:31:36.463407Z","iopub.status.idle":"2025-11-24T18:31:36.497002Z","shell.execute_reply.started":"2025-11-24T18:31:36.463391Z","shell.execute_reply":"2025-11-24T18:31:36.496238Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def canonical_smile(smile):\n    molecule = Chem.MolFromSmiles(smile)\n    canonical = Chem.MolToSmiles(molecule, canonical = True)\n    return canonical\n\ndef compute_all_descriptors(smiles):\n    mol = Chem.MolFromSmiles(smiles)\n    desc = []\n    for d in Descriptors.descList:\n        desc.append(d[1](mol))\n    return desc\n\ndef compute_graph_features(smiles, graph_feats):\n    mol = Chem.MolFromSmiles(smiles)\n    adj = rdmolops.GetAdjacencyMatrix(mol)\n    G = nx.from_numpy_array(adj)\n\n    graph_feats['graph_diameter'].append(nx.diameter(G) if nx.is_connected(G) else 0)\n    graph_feats['avg_shortest_path'].append(nx.average_shortest_path_length(G) if nx.is_connected(G) else 0)\n    graph_feats['num_cycles'].append(len(list(nx.cycle_basis(G))))\n\ndef preprocessing(df):\n    desc_names = [desc[0] for desc in Descriptors.descList]\n    descriptors = [compute_all_descriptors(smile) for smile in df['smiles'].to_list()]\n\n    graph_feats = {'graph_diameter': [], 'avg_shortest_path': [], 'num_cycles': []}\n    for smile in df['smiles']:\n        compute_graph_features(smile, graph_feats)\n\n    result = pd.concat([\n\t\t\tpd.DataFrame(descriptors, columns = desc_names),\n\t\t\tpd.DataFrame(graph_feats)\n\t\t],\n\t\taxis = 1\n\t)\n    result = result.replace([-np.inf, np.inf], np.nan)\n    return result","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:31:36.498442Z","iopub.execute_input":"2025-11-24T18:31:36.498732Z","iopub.status.idle":"2025-11-24T18:31:36.506692Z","shell.execute_reply.started":"2025-11-24T18:31:36.498706Z","shell.execute_reply":"2025-11-24T18:31:36.505945Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Visualising the Data\n\nThe boiling points (in K) and SMILES representations of 3790 diverse molecules were collected. The composition of the dataset is visualised below.\n\nThe distribution of boiling points is skewed with a tail towards higher boiling points.","metadata":{}},{"cell_type":"code","source":"binwidth=10\ndata = df_train['bp_K']\nplt.hist(data, bins=np.arange(min(data), max(data) + binwidth, binwidth), density=False)\nplt.xlabel('Boiling Point / K')\nplt.ylabel('Frequency')\nplt.show() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:31:36.507404Z","iopub.execute_input":"2025-11-24T18:31:36.507632Z","iopub.status.idle":"2025-11-24T18:31:36.777481Z","shell.execute_reply.started":"2025-11-24T18:31:36.507612Z","shell.execute_reply":"2025-11-24T18:31:36.776758Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"alcohol = 0; ketone = 0; alkene = 0; halogen = 0; amine = 0; aromatic = 0; \nnitrile = 0; alkyne = 0; alkane = 0; sulfide = 0; \n\nfor s in df_train['smiles']:\n    if 'N' in s: amine += 1\n    if 'O' in s: alcohol += 1\n    if 'c' in s: aromatic += 1\n    if 'C=C' in s: alkene += 1\n    if '#N' in s or 'N#' in s: nitrile += 1\n    elif '#' in s: alkyne += 1\n    if 'Br' in s or 'F' in s or 'Cl' in s: halogen += 1\n    if '=O' in s or 'O=' in s: ketone += 1\n    if 'S' in s: sulfide += 1\n    is_alkane = True\n    for i in 'NOc=#FBlS':\n        if i in s:\n            is_alkane = False\n            break\n    if is_alkane: alkane += 1\n\nlabels = ['Alcohol', 'Amine', 'Alkane', 'Halide', 'Ketone', 'Alkene', 'Aromatic', 'Nitrile', 'Sulfide']\nsizes = [alcohol, amine, alkane, halogen, ketone, alkene, aromatic, nitrile, sulfide]\nfig, ax = plt.subplots()\nax.pie(sizes, labels=labels)\nax.plot()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:31:36.77943Z","iopub.execute_input":"2025-11-24T18:31:36.779654Z","iopub.status.idle":"2025-11-24T18:31:36.902549Z","shell.execute_reply.started":"2025-11-24T18:31:36.779637Z","shell.execute_reply":"2025-11-24T18:31:36.901805Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Morgan Fingerprints\n\nMorgan extended connectivity fingerprints were constructed for each molecule using RDKit, using a radius of 3 and a length of 2048. Features that had a variance of less than 0.1% were dropped, resulting in a total of 1205 features for training. The correlation of these features with the boiling point is shown below.\n\nEvidently, most features in the fingerprints are only weakly correlated with the boiling point.\nThe total data was split into a training and testing set in a 9:1 split. An XGBoost model was fitted to the training data. Its hyperparameters were tuned using a Randomised CV Search with 5-fold cross validation. \n\nThe final model achieved a training score of 0.9470 and a testing score of 0.8117.\nA Random Forest Regressor was also fitted to the training data using 50 estimators. This achieved higher training and testing scores of 0.9612 and 0.8325, respectively.\n\nA plot of the prediction vs actual boiling points for the Random Forest model is given below. Lines have been drawn showing the +/- 10% confidence interval. ","metadata":{}},{"cell_type":"code","source":"# IMPORT DATASET - FINGERPRINT\ndf_train = (pd.read_csv('/kaggle/input/smiles-boiling-point/bp_data.csv'))\nprint(len(df_train))\n\nX = df_train.drop('bp_K', axis=1)\nfingerprint_length = 2**11\nfingerprint_cols = [[] for _ in range(fingerprint_length)]\nfor f in df_train['fingerprint']:\n    f = str(f)\n    zeros = fingerprint_length - len(f)\n    f = (\"0\" * zeros) + f \n    for i in range(fingerprint_length):\n        if f[i] == \"1\": fingerprint_cols[i].append(1)\n        else: fingerprint_cols[i].append(0)\n\n# CUT OFF COLUMNS WITH LOW VARIATION\nmin_spread = 0.001\nfor i in range(fingerprint_length):\n    if min_spread <= (fingerprint_cols[i].count(1) / len(fingerprint_cols[i])) <= 1 - min_spread:\n        X['fp_' + str(i)] = fingerprint_cols[i]\n\nSCALER = 2000\nX = X.copy()\nX = X.drop('smiles', axis=1).drop('fingerprint', axis=1)\nX['mw'] = np.log(X['mw'])\nX['valence e'] = np.log(X['valence e'])\ny = np.log(np.array(df_train['bp_K'].values / SCALER, dtype=float))\ninput_size = len(X.columns)\n_X = X.copy()\n_X['bp_K'] = np.log(df_train['bp_K'].values)\nprint(X.head())\n\nX_FINGERPRINT = X","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:31:36.903346Z","iopub.execute_input":"2025-11-24T18:31:36.90361Z","iopub.status.idle":"2025-11-24T18:31:39.804823Z","shell.execute_reply.started":"2025-11-24T18:31:36.903583Z","shell.execute_reply":"2025-11-24T18:31:39.804069Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IMPORT DATASET - DESCRIPTORS\ndf_train = (pd.read_csv('/kaggle/input/smiles-boiling-point/bp_descriptors.csv'))\n\nSMILES = df_train[\"smiles\"]\nX = df_train.drop('bp_K', axis=1).drop('smiles', axis=1)\n\n# CUT OFF COLUMNS WITH LOW VARIATION\nmin_spread = 0.01\nTO_DROP = []\nfor col in X.columns:\n    if max(X[col].value_counts()) / len(X[col]) >= 1 - min_spread: \n        TO_DROP.append(col)\n\nfor col in TO_DROP:\n    X = X.drop(col, axis=1)\n\nprint('Final Columns:', len(X.columns))\n\nscaler = StandardScaler()\n\nCOLUMNS = X.columns\nSCALER = 2000\nX = X.copy().select_dtypes(include=['float64', 'float32', 'int64', 'int32'])\ny = df_train['bp_K'].values / SCALER\n#y = np.log(np.array(df_train['bp_K'].values / SCALER, dtype=float))\ninput_size = len(X.columns)\n_X = X.copy()\n_X['bp_K'] = df_train['bp_K'].values\nprint(X.head())\n\n#for c in X_FINGERPRINT.columns.values:\n#    if c not in _X.columns.values:\n#        _X[c] = X_FINGERPRINT[c]\n\nX = np.array(X.values, dtype=float)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:31:39.805661Z","iopub.execute_input":"2025-11-24T18:31:39.806534Z","iopub.status.idle":"2025-11-24T18:32:11.131951Z","shell.execute_reply.started":"2025-11-24T18:31:39.80651Z","shell.execute_reply":"2025-11-24T18:32:11.131086Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Molecular Descriptors\n\nRDKit and Mordred were used to create 2048 2D and 3D descriptors for each molecule. Features that had a variance of less than 1% were dropped, resulting in a total of 1424 descriptors for training. The correlation of these features with the boiling point is shown below.\n\nThese descriptors are significantly more correlated with the boiling point than the features from the Morgan fingerprints. \n","metadata":{}},{"cell_type":"code","source":"numeric_cols = _X.select_dtypes(include=['int64', 'float64', 'float32', 'int32']).columns\nnumeric_cols = numeric_cols.drop('bp_K')\ncorr = _X[numeric_cols].corrwith(_X['bp_K']).sort_values(ascending=False)\n\nplt.figure(figsize=(12,6))\nsns.barplot(x=corr.index, y=corr.values)\nplt.xticks(rotation=90)\nplt.title(\"Correlation of numeric features with boiling point\")\nplt.xticks([], [])\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:32:11.132893Z","iopub.execute_input":"2025-11-24T18:32:11.133172Z","iopub.status.idle":"2025-11-24T18:32:14.064231Z","shell.execute_reply.started":"2025-11-24T18:32:11.133148Z","shell.execute_reply":"2025-11-24T18:32:14.063518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"SMILES = df_train[\"smiles\"]\n\ny = np.log(df_train['bp_K'].values)\n#y = df_train['bp_K'].values / SCALER\n\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nSMILES_TO_DESCRIPTORS = dict()\nfor row, s in zip(X, SMILES):\n    SMILES_TO_DESCRIPTORS[s] = row\ntrain_x, val_x, train_y, val_y = train_test_split(X, y, test_size=0.1, random_state=42)\n\ntrain_dataset = []\nfor a, b in zip(train_x, train_y):\n    train_dataset.append((a,b))\n\nval_dataset = []\nfor a, b in zip(val_x, val_y):\n    val_dataset.append((a,b))\n\nSMILES_TO_BP = dict()\nfor bp, s in zip(y, SMILES):\n    SMILES_TO_BP[s] = bp\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:36:36.860123Z","iopub.execute_input":"2025-11-24T18:36:36.860909Z","iopub.status.idle":"2025-11-24T18:36:36.927365Z","shell.execute_reply.started":"2025-11-24T18:36:36.860886Z","shell.execute_reply":"2025-11-24T18:36:36.926775Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from xgboost import XGBRegressor\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n\nxgb = XGBRegressor(\n    n_estimators=100,\n    learning_rate=0.05,\n    max_depth=4,\n    subsample=0.9,\n    colsample_bytree=0.9,\n    n_jobs=-1\n)\n\nparam_dist = {\n    'n_estimators':[200,300,400,500],\n    'max_depth': [3,4,5],\n    'learning_rate': [0.01, 0.05, 0.1],\n    'subsample': [0.7, 0.8, 0.9],\n    'colsample_bytree': [0.7, 0.8, 0.9]\n}\n\n\n#model = XGBRegressor()\n#xgb = RandomizedSearchCV(model, param_distributions=param_dist, cv=5, scoring='neg_mean_squared_error', n_jobs=-1, n_iter=10, verbose=True)\nxgb_model = xgb.fit(train_x, train_y)\n\nprint(xgb_model.score(train_x, train_y))\nprint(xgb_model.score(val_x, val_y))\n# Parameter which gives the best results\n#print(f\"Best Hyperparameters: {xgb.best_params_}\")\n# Accuracy of the model after using best parameters\n#print(f\"Best Score: {xgb.best_score_}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:32:14.127914Z","iopub.execute_input":"2025-11-24T18:32:14.128223Z","iopub.status.idle":"2025-11-24T18:32:16.394558Z","shell.execute_reply.started":"2025-11-24T18:32:14.12818Z","shell.execute_reply":"2025-11-24T18:32:16.393844Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The total data was again split into a training and testing set in a 9:1 split and an XGBoost model was fitted to the training data. Its hyperparameters were tuned using a Randomised CV Search with 5-fold cross validation. \n\nThe final model achieved a training score of 0.9891 and a testing score of 0.8156. This very high training score compared to the lower testing score could imply some overfitting for the model.\nSeveral Random Forest Regressors were also fitted to the training data using between 10 to 50 estimators. These all achieved higher testing scores than the XGBoost model, with the top performance using only 10 estimators (training score: 0.9585; testing score: 0.8683).","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n\ndef train_rf(n_estimators, train_x, train_y, random_state=0):\n    rf_model = RandomForestRegressor(n_estimators=n_estimators, random_state=random_state)\n    rf_model.fit(train_x, train_y)\n\n    return rf_model\n\ntrain_score = []\nval_score = []\nforest_size = [10, 20, 30, 40] # , 50, 60, 70]\nfor n in forest_size:\n    print(\"=== n_estimators: \" + str(n) + \" ===\")\n    rf = train_rf(n, train_x, train_y)\n    train_s = rf.score(train_x, train_y)\n    val_s = rf.score(val_x, val_y)\n    print(train_s, val_s)\n    train_score.append(train_s)\n    val_score.append(val_s)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:32:16.397087Z","iopub.execute_input":"2025-11-24T18:32:16.39732Z","iopub.status.idle":"2025-11-24T18:33:45.626537Z","shell.execute_reply.started":"2025-11-24T18:32:16.397302Z","shell.execute_reply":"2025-11-24T18:33:45.625877Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.plot(forest_size, train_score, label=\"training\")\nplt.plot(forest_size, val_score, label=\"validation\")\nplt.ylabel('Loss')\nplt.xlabel('Forest Size')\nplt.legend(loc='best')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:33:45.627309Z","iopub.execute_input":"2025-11-24T18:33:45.62756Z","iopub.status.idle":"2025-11-24T18:33:45.77414Z","shell.execute_reply.started":"2025-11-24T18:33:45.627543Z","shell.execute_reply":"2025-11-24T18:33:45.77344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"rf_model = train_rf(10, train_x, train_y) \nprint(\"Random Forest Regressor - 10 Estimators\")\nprint(\"Training Score: \", rf_model.score(train_x, train_y))\nprint(\"Validation Score: \", rf_model.score(val_x, val_y))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:36:09.159471Z","iopub.execute_input":"2025-11-24T18:36:09.160698Z","iopub.status.idle":"2025-11-24T18:36:18.852029Z","shell.execute_reply.started":"2025-11-24T18:36:09.160657Z","shell.execute_reply":"2025-11-24T18:36:18.85141Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"A plot of the prediction vs actual boiling points for the best performing Random Forest model is given below. Lines have been drawn showing the +/- 10% confidence interval. ","metadata":{}},{"cell_type":"code","source":"chosen_model = rf_model\n\nscatter_x = []\nscatter_y = []\nscatter_val_x = []\nscatter_val_y = []\nfor molecule, bp in tqdm(zip(train_x, train_y), total=min(len(train_x), len(train_y))):\n    molecule = molecule.reshape(1,-1)\n    pred_bp = chosen_model.predict(molecule)\n    scatter_x.append(pred_bp)\n    scatter_y.append(bp)\nfor molecule, bp in tqdm(zip(val_x, val_y), total=min(len(val_x), len(val_y))):\n    molecule = molecule.reshape(1,-1)\n    pred_bp = chosen_model.predict(molecule)\n    scatter_val_x.append(pred_bp)\n    scatter_val_y.append(bp)\n\nscatter_x = np.exp(scatter_x)\nscatter_y = np.exp(scatter_y)\nscatter_val_x = np.exp(scatter_val_x)\nscatter_val_y = np.exp(scatter_val_y)\n\nplt.scatter(scatter_x, scatter_y, label='training', s=5, alpha=0.5)\nplt.scatter(scatter_val_x, scatter_val_y, label='validation', s=5, alpha=0.5)\nplt.plot(scatter_y, scatter_y, c=\"black\", linewidth=0.5)\nplt.plot(scatter_y, 1.1* np.array(scatter_y), c=\"black\", linestyle=\"dashed\", linewidth=0.5)\nplt.plot(scatter_y, 0.9* np.array(scatter_y), c=\"black\", linestyle=\"dashed\", linewidth=0.5)\nplt.xlabel(\"Predicted BP / K\")\nplt.ylabel(\"Actual BP / K\")\nplt.legend(loc=\"best\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:37:00.38Z","iopub.execute_input":"2025-11-24T18:37:00.380659Z","iopub.status.idle":"2025-11-24T18:37:02.700123Z","shell.execute_reply.started":"2025-11-24T18:37:00.380635Z","shell.execute_reply":"2025-11-24T18:37:02.699145Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"molecules = ['CC(Cl)(Cl)Cl', 'O', 'N#Cc1ccccn1', 'C=CCN', 'CC(C)C', 'S=C=S', 'C']\nmolecules = ['SCCS', 'COC(CCl)=O', 'CCCCCCCC#CCCC', 'CCC[N+](=O)[O-]', 'CCC(C)C(SC1C)N=C1C']\n\nfor mol in molecules:\n    x = SMILES_TO_DESCRIPTORS[mol]\n    pred = np.exp(rf_model.predict([x])[0])\n    bp = np.exp(SMILES_TO_BP[mol])\n    diff = 100 * abs(bp - pred) / bp\n    print(f'{mol}. Predicted: {pred:.3f}. Actual: {bp:.3f}. Difference {diff:.1f}%')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:37:07.124385Z","iopub.execute_input":"2025-11-24T18:37:07.124667Z","iopub.status.idle":"2025-11-24T18:37:07.13612Z","shell.execute_reply.started":"2025-11-24T18:37:07.124647Z","shell.execute_reply":"2025-11-24T18:37:07.135388Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"DESCRIPTORS_TO_SMILES = dict()\nfor key, value in SMILES_TO_DESCRIPTORS.items():\n    DESCRIPTORS_TO_SMILES[tuple(value)] = key\ncnt = 0\nfor x in val_x:\n    cnt += 1\n    mol = DESCRIPTORS_TO_SMILES[tuple(x)]\n    x = SMILES_TO_DESCRIPTORS[mol]\n    pred = np.exp(rf_model.predict([x])[0])\n    bp = np.exp(SMILES_TO_BP[mol])\n    diff = 100 * abs(bp - pred) / bp\n    print(f'{mol}. Predicted: {pred:.3f}. Actual: {bp:.3f}. Difference {diff:.1f}%')\n    if cnt > 20: break","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:37:09.641808Z","iopub.execute_input":"2025-11-24T18:37:09.64244Z","iopub.status.idle":"2025-11-24T18:37:09.946516Z","shell.execute_reply.started":"2025-11-24T18:37:09.642418Z","shell.execute_reply":"2025-11-24T18:37:09.945754Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"94.4% of all molecules in the training and testing datasets scored a percentage difference smaller than 10%. Meanwhile, 84.8% of all molecules scored a percentage difference smaller than 5%, and 51.1% of all molecules scored a percentage difference smaller than 1%.","metadata":{}},{"cell_type":"code","source":"molecules = []\nSMILES_SET = set(SMILES)\nbelow_10 = 0\nfor mol in tqdm(SMILES_SET):\n    x = SMILES_TO_DESCRIPTORS[mol]\n    pred = np.exp(rf_model.predict([x])[0])\n    bp = np.exp(SMILES_TO_BP[mol])\n    diff = 100 * abs(bp - pred) / bp\n    if diff < 10: below_10 += 1\n    molecules.append((diff, pred, bp, mol))\nmolecules = sorted(molecules)[::-1]\nfor i in range(10):\n    print(molecules[i])\nprint(below_10, below_10 / len(molecules))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:37:12.841718Z","iopub.execute_input":"2025-11-24T18:37:12.842004Z","iopub.status.idle":"2025-11-24T18:37:14.768404Z","shell.execute_reply.started":"2025-11-24T18:37:12.841983Z","shell.execute_reply":"2025-11-24T18:37:14.767435Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Model Insights\n\nPermutation importance and SHAP values were used to uncover insights into the trained Random Forest Regressor. \n\nThe SHAP force plots demonstrate how each descriptor impacted the final model prediction, examples of which are given below.","metadata":{}},{"cell_type":"code","source":"feature_importances = pd.DataFrame({\n    'feature': numeric_cols,\n    'importance': xgb_model.feature_importances_\n}).sort_values('importance', ascending=False)\n\n# Get top 20 most important features\ntop_20_features = feature_importances.head(20)\n\n# Visualize the results\nplt.figure(figsize=(12, 8))\nsns.barplot(x='importance', y='feature', data=top_20_features)\nplt.title('Top 20 Feature Importances of XGBoost Model')\nplt.xlabel('Importance')\nplt.ylabel('Features')\nplt.tight_layout()\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:37:17.044468Z","iopub.execute_input":"2025-11-24T18:37:17.044784Z","iopub.status.idle":"2025-11-24T18:37:17.400632Z","shell.execute_reply.started":"2025-11-24T18:37:17.044759Z","shell.execute_reply":"2025-11-24T18:37:17.399612Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import shap\n\n# Create object that can calculate shap values\nexplainer = shap.TreeExplainer(rf_model)\n\n# Calculate Shap values\nshap_values = explainer.shap_values(val_x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:37:20.317439Z","iopub.execute_input":"2025-11-24T18:37:20.31796Z","iopub.status.idle":"2025-11-24T18:37:29.143221Z","shell.execute_reply.started":"2025-11-24T18:37:20.317936Z","shell.execute_reply":"2025-11-24T18:37:29.14259Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"shap.initjs()\nshap.force_plot(explainer.expected_value[0], shap_values[2], val_x[2], feature_names=numeric_cols.values.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:37:41.248749Z","iopub.execute_input":"2025-11-24T18:37:41.24904Z","iopub.status.idle":"2025-11-24T18:37:41.27057Z","shell.execute_reply.started":"2025-11-24T18:37:41.249018Z","shell.execute_reply":"2025-11-24T18:37:41.269644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Permutation importance can be used to rank the importance of each individual feature in the model as a whole. The top 10 features and their relative weights are given below.\n\nThe most important descriptors identified by both the SHAP values and permutation importance are difficult to interpret as they correspond to abstract structural features of the molecules. ","metadata":{}},{"cell_type":"code","source":"import eli5\nfrom eli5.sklearn import PermutationImportance\nperm = PermutationImportance(rf_model, random_state=1).fit(val_x, val_y)\neli5.show_weights(perm, feature_names = numeric_cols.values.tolist())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-24T18:37:54.364562Z","iopub.execute_input":"2025-11-24T18:37:54.365299Z","iopub.status.idle":"2025-11-24T18:38:02.108519Z","shell.execute_reply.started":"2025-11-24T18:37:54.365267Z","shell.execute_reply":"2025-11-24T18:38:02.107728Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\n\nSMILES representations of molecules can be used to generate molecular fingerprints and descriptors, both of which can be used to predict molecular properties with a reasonable accuracy. \n\nMolecular descriptors include more detailed information about the molecule and are more strongly correlated with boiling point. This translates to a more accurate performance on the testing dataset. ","metadata":{}}]}